version: 2.1


executors:
  python-js:
    docker:
      - image: circleci/python:3.8-node-browsers
    working_directory: ~/project


jobs:
  install:
    executor: python-js
    steps:
      - checkout
      - run:
          name: Install fonts
          command: |
            echo "deb http://ftp.us.debian.org/debian buster main contrib" | sudo tee -a /etc/apt/sources.list
            echo ttf-mscorefonts-installer msttcorefonts/accepted-mscorefonts-eula select true | sudo debconf-set-selections
            sudo apt-get update
            while ! sudo apt-get install ttf-mscorefonts-installer
            do
              echo "Fonts installation failed! Retrying..."
            done
      - run:
          name: Install tor
          command: sudo apt-get install tor tor-geoipdb
      - restore_cache:  # caches npm deps based on whether the deps declaration has changed
          key: deps-{{ checksum "Pipfile.lock" }}-{{ checksum "package-lock.json" }}
      - run:
          name: Install project dependencies
          command: pipenv install --dev && npm install
      - save_cache:
          key: deps-{{ checksum "Pipfile.lock" }}-{{ checksum "package-lock.json" }}
          paths:
            - ~/.local/share/virtualenvs
            - ./node_modules
      - persist_to_workspace:
          root: "~"
          paths:
              - .local/share/virtualenvs
              - project/node_modules
              - project

  test:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run test -v

  lint:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run lint

  # needs $GOOGLE_SERVICE_ACCOUNT, $FIOBANK_API_KEY
  fetch:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: cp tor.cfg ~/.torrc
      - run: |
          ls -al /etc/init.d/
          sudo /etc/init.d/tor start
      - run: 'date +"%Y-%m-%d" > .today'  # saves today's date to a file so it can be used to expire cache
      - restore_cache:  # caches all HTTP communication of the scrapers for one day
          key: scrapy-{{ checksum ".today" }}
      - run: pipenv run fetch
      - save_cache:
          key: scrapy-{{ checksum ".today" }}
          paths:
            - .scrapy
      - persist_to_workspace:
          root: "~"
          paths:
              - project/juniorguru/data
      - run: tar -cvzf backup.tar.gz ./juniorguru/data
      - store_artifacts:
          path: ./backup.tar.gz  # all data gets backed up as a CI artifact

  build:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run build
      - persist_to_workspace:
          root: "~"
          paths:
              - project/public
      - run: tar -cvzf public.tar.gz ./public
      - store_artifacts:
          path: ./public.tar.gz  # the generated HTML is made available for download (used in the deploy job)

  # needs $NOW_TOKEN, $NOW_ORG_ID, $NOW_PROJECT_ID
  deploy:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - restore_cache:  # caches a dummy file .deploy which lets the subsequent code to recognize whether the git ref has been already deployed to Now or not
          key: deploy-{{ .Revision }}
      - run:
          name: Deploy
          command: |
            if [[ -f .deploy ]]; then
              echo "Looks like the build has been triggered by CircleCI (nightly, retries). Deploying with ZEIT Now CLI."
              sudo npm install --global --unsafe-perm now
              now --prod -t "$NOW_TOKEN"
            else
              echo "Looks like the build has been triggered by GitHub (there are no previous 'deploy' jobs for this commit). Skipping duplicate deploy with ZEIT Now CLI."
              echo "$CIRCLE_SHA1" > .deploy
            fi
      - save_cache:
          key: deploy-{{ .Revision }}
          paths:
            - .deploy

  # needs $SMTP_* (see readme)
  send:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: |
          export LOG_LEVEL=debug
          export SMTP_ENABLED=1
          pipenv run send

  check-anchors:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run check-anchors

  check-links:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run check-links --retry

  check-scrapers:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run check-scrapers

  check-performance:
    executor: python-js
    steps:
      - attach_workspace:
          at: "~"
      - run: pipenv run check-performance
      - run:
          command: tar -cvzf lighthouse.tar.gz ./lighthouse
          when: always
      - store_artifacts:
          path: ./lighthouse.tar.gz  # HTML and JSON reports from lighthouse


workflows:
  version: 2

  push:
    jobs:
      - install
      - test:
          requires:
            - install
      - lint:
          requires:
            - install
      - fetch:
          requires:
            - install
      - build:
          requires:
            - fetch
      - check-scrapers:
          requires:
            - fetch
      - check-anchors:
          requires:
            - build
      - deploy:
          # This makes sure retries of 'master' builds still deploy,
          # even though they were not initiated by a GitHub push. Might
          # cause double deploy for builds initiated by GitHub push, but
          # the deploy job recognizes they're to be skipped.
          filters:
            branches:
              only:
                - master
          requires:
            - build

  nightly:
    jobs:
      - install
      - fetch:
          requires:
            - install
      - build:
          requires:
            - fetch
      - check-scrapers:
          requires:
            - fetch
      - check-links:
          requires:
            - build
      - deploy:
          requires:
            - build
      - check-performance:
          requires:
            - deploy
    triggers:
      - schedule:
          cron: "0 4 * * *"
          filters:
            branches:
              only:
                - master

  sending:
    jobs:
      - install
      - fetch:
          requires:
            - install
      - send:
          requires:
            - fetch
    triggers:
      - schedule:
          cron: "0 5 * * *"
          filters:
            branches:
              only:
                - master
